import numpy as np
import torch
from torch.nn import functional as F
import os
from src.set_encoder import SetEncoder


##################ç”¨æ¥åŠ è½½xå’Œyçš„å‡½æ•°##################
def load_xy(folder_path, device="cpu"):
    """
    åªè¯»å– x, y
    x: (N, Dx)
    y: (N, 1) or (N,)
    """
    x = np.load(os.path.join(folder_path, "x.npy"))
    y = np.load(os.path.join(folder_path, "y.npy"))

    return (
        torch.tensor(x, dtype=torch.float32, device=device),
        torch.tensor(y, dtype=torch.float32, device=device),
    )


class Retriever:
    def __init__(self, set_encoder, device="cpu", normalize_embedding=True):
        self.set_encoder = set_encoder.to(device)
        self.device = device

        self.normalize_embedding = normalize_embedding

        self.embeddings = None      # (N, D)
        self.dataset_ids = []       # list[str]



    ########################################
    # ===== å•ä¸ª embeddingï¼ˆno_gradï¼‰
    ########################################
    def get_embedding(self, x, y):
        if x.dim() == 2:
            x = x.unsqueeze(0)
        if y.dim() == 1:
            y = y.unsqueeze(0)
        elif y.dim() == 2:
            y = y.unsqueeze(0)

      
        emb = self.set_encoder(x, y)

        if self.normalize_embedding:
                emb = F.normalize(emb, dim=-1)

        return emb.squeeze(0)

    ########################################
    # ===== æ‰¹é‡ embedding
    ########################################
    def get_embedding_batch(self, x_batch, y_batch):
        """
        x_batch: (B, 100, 16)
        y_batch: (B, 100)
        """
        
        emb = self.set_encoder(x_batch, y_batch)
        if self.normalize_embedding:
                emb = F.normalize(emb, dim=-1)
        return emb

    ########################################
    # ===== æ”¯æŒ batch_size
    ########################################
    def build_index(self, root_dir, batch_size=11):
        self.set_encoder.eval()

        subdirs = sorted(
            d for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        )

        embeddings = []
        self.dataset_ids = []

        total = len(subdirs)
        print(f"ğŸ“¦ Building embeddings for {total} datasets")

        for start in range(0, total, batch_size):
            end = min(start + batch_size, total)
            batch_dirs = subdirs[start:end]

            x_list, y_list = [], []

            for dataset_id in batch_dirs:
                folder = os.path.join(root_dir, dataset_id)
                x, y = load_xy(folder, device=self.device)
                x_list.append(x)
                y_list.append(y)
                self.dataset_ids.append(dataset_id)

            x_batch = torch.stack(x_list, dim=0).to(self.device)
            y_batch = torch.stack(y_list, dim=0).to(self.device)

            emb_batch = self.get_embedding_batch(x_batch, y_batch)
            embeddings.append(emb_batch.cpu())

            print(f"  ğŸš€ {end}/{total} processed")

        self.embeddings = torch.cat(embeddings, dim=0)
        print("âœ… Embedding index built.")

#################ä¿å­˜æ€»çš„embeddingsyä»¥åŠç›¸åº”çš„æ•°æ®é›†åç§°##################
    def save_index(self, save_path):
        assert self.embeddings is not None, "Embeddings not built yet!"

        save_obj = {
        "embeddings": self.embeddings.cpu(),  # ğŸ”´  ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´å­˜ CPU
        "dataset_ids": self.dataset_ids,
        "normalize_embedding": self.normalize_embedding,
      }

        torch.save(save_obj, save_path)
        print(f"âœ… Embedding index saved to: {save_path}")

###################å¦‚æœå·²ç»æœ‰embeddings,åˆ™ç›´æ¥å°±åŠ è½½##################
    def load_index(self, load_path):
        ckpt = torch.load(load_path, map_location=self.device)

        self.embeddings = ckpt["embeddings"].to(self.device)
        self.dataset_ids = ckpt["dataset_ids"]
        self.normalize_embedding = ckpt.get(
        "normalize_embedding", True
        )

        print(
          f"âœ… Loaded {self.embeddings.shape[0]} embeddings "
          f"of dim {self.embeddings.shape[1]}"
      )

######################é€‰æ‹©å‰kä¸ªæœ€è¿‘çš„dataset_id######################
    def retrieve_top_k_ids(self, x, y, k=10):
        """
        è¾“å…¥æ–° dataset (x, y)
        è¾“å‡ºï¼šæœ€è¿‘çš„ k ä¸ª dataset_idï¼ˆå­æ–‡ä»¶å¤¹åï¼‰
        """
        # (D,)
        query_emb = self.get_embedding(x, y)

        # (1, D)
        query_emb = query_emb.unsqueeze(0)

        # squared L2 distance
        # embeddings: (N, D)
        dist = ((self.embeddings - query_emb) ** 2).sum(dim=1)

        _, topk_idx = torch.topk(dist, k, largest=False)

        topk_ids = [self.dataset_ids[i] for i in topk_idx.tolist()]
        return topk_ids


#####################æ™ºèƒ½å‡†å¤‡ Retriever çš„ embedding index#####################
def prepare_retriever_index(retriever, root_dir, index_path):
    """
    æ™ºèƒ½å‡†å¤‡ Retriever çš„ embedding indexï¼š
    - å¦‚æœå·²æœ‰ symtab_embedding_index.ptï¼Œç›´æ¥åŠ è½½
    - å¦åˆ™éå†æ•°æ®é›†è®¡ç®— embedding å¹¶ä¿å­˜
    """

    if os.path.exists(index_path):
        print(f"âš¡ Found existing embedding index: {index_path}. Loading...")
        retriever.load_index(index_path)
    else:
        print(f"âš ï¸ No existing index found. Building embeddings from {root_dir} ...")
        retriever.build_index(root_dir)
        retriever.save_index(index_path)
        print(f"âœ… Embedding index built and saved to: {index_path}")


# ============================================ ä½¿ç”¨ç¤ºä¾‹ ====================================================
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    #åˆå§‹åŒ–set encoderå’Œretriever
    set_encoder = SetEncoder(
        num_x_features=16,
        n_out=512,
        nhead=4,
        nhid=1024,
        nlayers=6,
        dropout=0.0,
    )

    retriever = Retriever(
        set_encoder,
        device=device,
        normalize_embedding=True,
    )

    root_dir = "/Users/zikaixie/PycharmProjects/TabPFN/sympfn_data" # å­˜å‚¨å¤šä¸ªå­æ•°æ®é›†çš„æ ¹ç›®å½•
    index_path = os.path.join(root_dir, "symtab_embedding_index.pt") # embedding index ä¿å­˜è·¯å¾„

    # æ™ºèƒ½åŠ è½½ / æ„å»º embedding
    prepare_retriever_index(retriever, root_dir, index_path)

    # æ–°æ•°æ®æ£€ç´¢
    x_new, y_new = load_xy(
        "/Users/zikaixie/PycharmProjects/TabPFN/sympfn_data/dataset1_17",
        device="cpu"
    )

    nearest_ids = retriever.retrieve_top_k_ids(x_new, y_new, k=3)
    print(nearest_ids)